## 特征工程概述

+ 基本步骤
  + 特征抽取
  + 数据特征的预处理(重点)
  + 特征选择
+ 为什么需要特征工程
  - 样本数据中的特征有可能会存在缺失值，重复值，异常值等等，那么我们是需要对特征中的相关的噪点数据进行处理的，那么处理的目的就是为了营造出一个更纯净的样本集，让模型基于这组数据可以有更好的预测能力。当然特征工程不是单单只是处理上述操作！
  - 比如AlphaGo学习的数据中既有棋谱，又有食谱还有歌词，那么一些干扰的数据绝对会影响AlphaGo的学习。

- 什么是特征工程

  > 特征工程是将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对未知数据预测的准确性。

- 特征工程的意义

  > 直接影响模型预测的结果

- 如何实现特征工程

  > 工具：sk-learn

- sklean介绍
  - 是python语言中的机器学习工具，包含了很多知名的机器学习算法的实现，其文档完善，容易上手。
  - 功能
    - 分类模型
    - 回归模型
    - 聚类模型
    - 特征工程

## 特征抽取

- 目的

  > 我们所采集到样本中的特征数据往往很多时候为字符串或者其他类型的数据，我们知道电脑只可以识别二进制数值型的数据，如果把字符串给电脑，电脑是看不懂的。机器学习学习的数据如果不是数值型的数据，它是识别不了的。

- 效果演示--将字符串转换成数字

  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  vector = CountVectorizer()
  res = vector.fit_transform(['lift is short,i love python','lift is too long,i hate python'])
  print(res.toarray())
  -----------
  [[0 1 1 0 1 1 1 0]
   [1 1 1 1 0 1 0 1]]
  ```

- 演示后的结论

  > 特征抽取对文本等数据进行特征值化。特征值化是为了让机器更好的理解数据。

### 字典特征抽取

- 作用：对字典数据进行特征值化

- API：from sklearn.feature_extraction import DictVectorizer

  - fit_transform(X):X为**字典或者包含字典**的迭代器，返回值为sparse矩阵(*稀疏矩阵*)
  - inverse_transform(X)：X为sparse矩阵或者array数组，返回值为转换之前的数据格式
  - transform(X)：按照原先的标准转换
  - get_feature_names()：返回类别名称

  ```python
  from sklearn.feature_extraction import DictVectorizer
  alist = [
              {'city':'BeiJing','temp':33},
              {'city':'GZ','temp':42},
              {'city':'SH','temp':40}
          ]
  d = DictVectorizer() #实例化工具对象
  result = d.fit_transform(alist) #使用工具将字典中的非数值型数据转换数值型的数据
  #result是一个sparse矩阵
  print(d.get_feature_names())
  print(result)
  -------------------
  ['city=BeiJing', 'city=GZ', 'city=SH', 'temp']
    (0, 0)	1.0
    (0, 3)	33.0
    (1, 1)	1.0
    (1, 3)	42.0
    (2, 2)	1.0
    (2, 3)	40.0
  ```

- 什么是sparse矩阵如何理解？

  - 在DictVectorizer类的构造方法中中设定sparse=False则返回的就不是sparse矩阵，而是一个数组。
  - sparse矩阵就是一个变相的数组或者列表，目的是为了节省内存

  ```python
  from sklearn.feature_extraction import DictVectorizer
  alist = [
              {'city':'BeiJing','temp':33},
              {'city':'GZ','temp':42},
              {'city':'SH','temp':40}
          ]
  d = DictVectorizer(sparse=False) #实例化工具对象
  #sparse=False表示转换后的结果是一个数组而不是sparse矩阵
  result = d.fit_transform(alist)
  print(d.get_feature_names())
  print(result)
  print(d.inverse_transform(result))
  ------------------
  ['city=BeiJing', 'city=GZ', 'city=SH', 'temp']
  [[ 1.  0.  0. 33.]
   [ 0.  1.  0. 42.]
   [ 0.  0.  1. 40.]]
  [{'city=BeiJing': 1.0, 'temp': 33.0}, {'city=GZ': 1.0, 'temp': 42.0}, {'city=SH': 1.0, 'temp': 40.0}]
  ```

- 特征值化

  > 将特征中非数值型的数据转换成数值型的数据

- OneHot编码

  > sparse矩阵中的0and1就是onehot编码

  ![1586761276423](asserts/1586761276423.png)

- 为什么需要onehot编码呢？

  - 特征抽取主要目的就是对非数值型的数据进行特征值化！如果现在需要对下图中的human和alien进行手动特征值化Alien为4，human为1。则1和4有没有优先级或者权重大小之分呢？

    ![1586761323779](asserts/1586761323779.png)

  - 则就需要对其进行One-Hot编码

    ![1586761346291](asserts/1586761346291.png)

- 基于pandas实现one-hot编码

  - pd.get_dummies(df['col'])

  - 示例

    - 基本数组

      ```python
      import pandas as pd
      df = pd.DataFrame([
       ['green', 'M', 20, 'class1'],
       ['red', 'L', 21, 'class2'],
       ['blue', 'XL',30, 'class3']])
      df.columns = ['color', 'size', 'weight', 'class_label']
      df
      -------------
      
      	color	size	weight	class_label
      0	green	M		20		class1
      1	red		L		21		class2
      2	blue	XL		30		class3
      ```

    - 特征值化

      ```python
      df1 = pd.get_dummies(df['color'])
      df1
      ----------------
      	blue	green	red
      0	0		1		0
      1	0		0		1
      2	1		0		0
      ```

    - 修改原数组

      ```python
      pd.concat((df,df1),axis=1).drop(labels='color',axis=1)
      -----------
      	size	weight	class_label	blue	green	red
      0	M		20		class1		0		1		0
      1	L		21		class2		0		0		1
      2	XL		30		class3		1		0		0
      ```

### 文本特征抽取

- 作用：对文本数据进行特征值化---基于类别名称进行对应计数统计

  - API:from sklearn.feature_extraction.text import CountVectorizer
  - fit_transform(X):X为文本或者包含文本字符串的可迭代对象，返回sparse矩阵
  - inverse_transform(X)：X为array数组或者sparse矩阵，返回转换之前的格式数据
  - get_feature_names()
  - toarray()：将sparse矩阵换成数组

  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  
  text_list = ['left is is short i love python','left is too long,i hate python']
  c = CountVectorizer()
  result = c.fit_transform(text_list)
  #使用toarray方法将sparse矩阵转换成数组
  arr_res = result.toarray()
  print(c.get_feature_names()) #从文本中提取出来的非数值型的特征数据
  print(arr_res)
  ----------------
  ['hate', 'is', 'left', 'long', 'love', 'python', 'short', 'too']
  [[0 2 1 0 1 1 1 0]
   [1 1 1 1 0 1 0 1]]
  ```

  **注意：单字符的单词是不做特征抽取**

- 中文文本特征抽取

  - 对有标点符号的中文文本进行特征抽取

    ```python
    text_list = ['我喜欢你的鼻子眼睛','我想你了，你有没有也在想我呢？']
    from sklearn.feature_extraction.text import CountVectorizer
    c = CountVectorizer()
    result = c.fit_transform(text_list)
    #使用toarray方法将sparse矩阵转换成数组
    arr_res = result.toarray()
    print(c.get_feature_names()) #从文本中提取出来的非数值型的特征数据
    print(arr_res)
    -------------
    ['你有没有也在想我呢', '我喜欢你的鼻子眼睛', '我想你了']
    [[0 1 0]
     [1 0 1]]
    ```

  - 对有标点符合且有空格分隔的中文文本进行特征处理

    ```python
    text_list = ['我 喜欢你,的鼻子-眼睛','我-想你-了，你-有没有,也在 想我呢？']
    from sklearn.feature_extraction.text import CountVectorizer
    c = CountVectorizer()
    result = c.fit_transform(text_list)
    #使用toarray方法将sparse矩阵转换成数组
    arr_res = result.toarray()
    print(c.get_feature_names()) #从文本中提取出来的非数值型的特征数据
    print(arr_res)
    ----------------
    ['也在', '喜欢你', '想你', '想我呢', '有没有', '的鼻子', '眼睛']
    [[0 1 0 0 0 1 1]
     [1 0 1 1 1 0 0]]
    ```

  - 结论

    > 目前CountVectorizer只可以对有标点符号和用分隔符对应的文本进行特征抽取，显然这是满足不了我们日常需求的，因为在自然语言处理中，我们是需要将一段中文文本中相关的词语，成语，形容词......都要进行抽取的

- jieba分词

  - 对中文文章进行分词处理

  - pip install jieba

  - jieba分词的基本使用

    ```python
    import jieba
    text = '我喜欢你的鼻子眼睛，我想你了，你有没有也在想我呢？'
    jb = jieba.cut(text) #分词结果，分词结果需要转成list才可以显示
    jb_text = list(jb)
    print(jb_text)
    -----------
    ['我', '喜欢', '你', '的', '鼻子眼睛', '，', '我', '想', '你', '了', '，', '你', '有没有', '也', '在', '想', '我', '呢', '？']
    ```

  - jieba分词+CountVectorizer工具类实现对文本数据的特征抽取

    ```python
    #使用jieba分词+CountVectorizer工具类实现对文本数据的特征抽取
    import jieba
    text = '我喜欢你的鼻子眼睛，我想你了，你有没有也在想我呢？'
    jb = jieba.cut(text) #分词结果，分词结果需要转成list才可以显示
    text = '-'.join(list(jb))
    print(text) #分词后的结果，结果中每一个词都使用了分隔符进行分割
    c = CountVectorizer()
    result = c.fit_transform([text])
    print(c.get_feature_names())
    print(result.toarray())
    -------------
    我-喜欢-你-的-鼻子眼睛-，-我-想-你-了-，-你-有没有-也-在-想-我-呢-？
    ['喜欢', '有没有', '鼻子眼睛']
    [[1 1 1]]
    ```

    **注意，在中文中也是不会对单个的汉字进行特征值化的。**

    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    
    text_list = ['从“遭遇战”到“阻击战”','从“重中之重”到“人民战争”','从“头等大事”到“全面胜利”']
    c = CountVectorizer()
    result = c.fit_transform(list(map('-'.join,map(jieba.cut,text_list))))
    #使用toarray方法将sparse矩阵转换成数组
    arr_res = result.toarray()
    print(c.get_feature_names()) #从文本中提取出来的非数值型的特征数据
    print(arr_res)
    ---------------
    ['人民战争', '全面', '头等大事', '胜利', '遭遇战', '重中之重', '阻击战']
    [[0 0 0 0 1 0 1]
     [1 0 0 0 0 1 0]
     [0 1 1 1 0 0 0]]
    ```

## 特征的预处理：对数值型的数据进行处理

+ 案例分析

  ![1586762897194](asserts/1586762897194.png)

+ 无量纲化

  - 在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布的需求这种需求统称为将数据“无量纲化”。譬如梯度和矩阵为核心的算法中，譬如逻辑回归，支持向量机，神经网络，无量纲化可以加快求解速度;而在距离类模型，譬如K近邻，K-Means聚类中，无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。(一个特例是决策树和树的集成算法们，对决策 树我们不需要无量纲化，决策树可以把任意数据都处理得很好。)
  - 那么预处理就是用来实现无量纲化的方式。

+ 含义：特征抽取后我们就可以获取对应的数值型的样本数据啦，然后就可以进行数据的预处理了。

+ 概念：通过特定的统计方法（数学方法），将数据转换成算法要求的数据

+ 方式

  - 归一化
    - 优点：计算复杂度较低。
    - 缺点：对异常值过于敏感。
  - 标准化
    - 优点：对异常值不敏感
    - 缺点：计算复杂度较高于归一化

### 归一化的实现

![1586762976632](asserts/1586762976632.png)

+ 归一化后的数据服从正太分布。

+ API:from sklearn.preprocessing import MinMaxScaler

  - 参数：feature_range表示缩放范围，通常使用(0,1)
  - fit_transform(X):归一化操作
  - inverse_transform(X)：将归一化之后的结果转换成归一化之前的结果

  ```python
  data = [[90,2,10,40],[60,5,15,45],[73,3,13,45]]
  data = np.array(data)
  print(data)
  from sklearn.preprocessing import MinMaxScaler
  mm = MinMaxScaler()
  result = mm.fit_transform(data)
  print(result)
  print(mm.inverse_transform(result))
  --------------
  [[90  2 10 40]
   [60  5 15 45]
   [73  3 13 45]]
  
  [[1.         0.         0.         0.        ]
   [0.         1.         1.         1.        ]
   [0.43333333 0.33333333 0.6        1.        ]]
  
  [[90.  2. 10. 40.]
   [60.  5. 15. 45.]
   [73.  3. 13. 45.]]
  ```

+ 作用：使得某一个特征对最终结果不会造成很大的影响

+ 问题：如果数据中存在的异常值比较多，会对结果造成什么样的影响？

  > 结合着归一化计算的公式可知，异常值对原始特征中的最大值和最小值的影响很大，因此也会影响对归一化之后的值。这个也是归一化的一个弊端，无法很好的处理异常值。

+ 归一化总结

  > 在特定场景下最大值和最小值是变化的，另外最大最小值很容易受到异常值的影响，所以这种归一化的方式具有一定的局限性。因此引出了一种更好的方式叫做：标准化！！！

### 标准化实现

+ 标准化的处理

  - 当数据按均值中心化后，再按标准差缩放，数据就会服从为均值为0，方差为1的正态分布(即标准正态分布)，而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)，公式如下

    ![1586763361721](asserts/1586763361721.png)

  - 从公式中可以看出，异常值对均值和标准差的影响不大

+ 归一化和标准化总结

  - 对于归一化来说，如果出现了异常值则会响应特征的最大最小值，那么最终结果会受到比较大影响
  - 对于标准化来说，如果出现异常点，由于具有一定的数据量，少量的异常点对于平均值的影响并不大，从而标准差改变比较少。

- StandardScaler和MinMaxScaler选哪个?

  - 看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。
  - 建议先试试看StandardScaler，效果不好换MinMaxScaler。
  - 拓展：在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。 MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。

- API

  - 处理后，每列所有的数据都聚集在均值为0，标准差为1范围附近

  - 标准化API:from sklearn.preprocessing import StandardScaler

    - fit_transform(X):对X进行标准化
    - mean_：均值
    - var_:方差

    ```python
    data = [[90,2,10,40],[60,5,15,45],[73,3,13,45]]
    data = np.array(data)
    from sklearn.preprocessing import StandardScaler
    s = StandardScaler()
    result = s.fit_transform(data)
    print(result)
    print(s.inverse_transform(result))
    ------------------
    [[ 1.27540458 -1.06904497 -1.29777137 -1.41421356]
     [-1.16685951  1.33630621  1.13554995  0.70710678]
     [-0.10854507 -0.26726124  0.16222142  0.70710678]]
    
    [[90.  2. 10. 40.]
     [60.  5. 15. 45.]
     [73.  3. 13. 45.]]
    ```

    

## 特征选择

+ 目标

  > 从特征中选择出有意义对模型有帮助的特征作为最终的机器学习输入的数据！

- 切记
  - 在做特征选择之前，有三件非常重要的事:跟数据提供者联系，跟数据提供者沟通，跟数据提供者开会。
  - 一定要抓住给你提供数据的人，尤其是理解业务和数据含义的人，跟他们聊一段时间。技术能够让模型起飞，前提是你和业务人员一样理解数据。所以特征选择的第一步，其实是根据我们的目标，用业务常识来选择特征。
- 特征选择的原因
  - 冗余：部分特征的相关度高，容易消耗计算机的性能
  - 噪点：部分特征对预测结果有偏执影响
- 特征选择的实现
  - 人为对不相关的特征进行主观舍弃
  - 当然了，在真正的数据应用领域，比如金融，医疗，电商，我们的数据特征非常多，这样明显，那如果遇见极端情况，我们无法依赖对业务的理解来选择特征，该怎么办呢?
    - 在已有特征和对应预测结果的基础上，使用相关的工具过滤掉一些无用或权重较低的特征
      - 工具：
        - Filter（过滤式）【主要讲解】
        - Embedded（嵌入式）：决策树模型会自己选择出对其重要的特征。【后期在讲解模型的时候再补充】
        - PCA降维

### 过滤式

- Filter过滤式（方差过滤）

  - 原理：这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0或者方差极低的特征。
  - API:from sklearn.feature_selection import VarianceThreshold
  - VarianceThreshold(threshold=x)threshold方差的值，删除所有方差低于x的特征，默认值为0表示保留所有方差为非0的特征
  - fit_transform(X)#:X为特征

  ```python
  data = [[90,2,10,40],[60,5,15,45],[73,3,13,45]]
  data = np.array(data)
  from sklearn.feature_selection import VarianceThreshold
  v = VarianceThreshold(threshold=5) #方差小于5的特征过滤
  v.fit_transform(data)
  -----------
  array([[90, 40],
         [60, 45],
         [73, 45]])
  ```

- 如果将方差为0或者方差极低的特征去除后，剩余特征还有很多且模型的效果没有显著提升,则方差也可以帮助我们将特征选择【一步到位】。留下一半的 特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数 threshold的值输入就好了。

  - VarianceThreshold(np.median(X.var().values)).fit_transform(X)

    - X为样本数据中的特征列

    ```python
    from sklearn.feature_selection import VarianceThreshold
    v = VarianceThreshold(np.median(data.var(axis=0)))
    v.fit_transform(data)
    --------------
    array([[90, 40],
           [60, 45],
           [73, 45]])
    ```

- 方差过滤对模型的影响

  > 我们这样做了以后，对模型效果会有怎样的影响呢?在这里，我为大家准备了KNN在方差过滤前和方差过滤后运行的效果和运行时间的对比。KNN是K近邻算法中的分类算法，其原理非常简单，是利用每个样本到其他样本点的距离来判断每个样本点的相似度，然后对样本进行分类。KNN必须遍历每个特征和每个样本，因而特征越多，KNN的计算也就会越缓慢。由于这一段代码对比运行时间过长，所以我为大家贴出了代码和结果。

  + 方差过滤前

    ![1586764117601](asserts/1586764117601.png)

  + 方差过滤后

    ![1586764127806](asserts/1586764127806.png)

  + 可以看出，对于KNN，过滤后的效果十分明显:准确率稍有提升，但平均运行时间减少了10分钟，特征选择过后算法的效率上升了1/3.

  + **注意**

    > 方差过滤主要服务的对象是：需要遍历特征的算法模型。过滤法的主要目的是:在维持算法表现的前提下，帮助算法们降低计算成本。

### PCA降维（主成分分析）

- 降维的维度值的就是特征的种类。----是一种分析，简化数据集的技术

- 思想：如何最好的对一个立体的物体用二维表示

  ![1586764265192](asserts/1586764265192.png)

- 当然，第四张二维图片可以比较好的标识一个立体三维的水壶。但是也要清楚，用一个低纬度去表示高纬度的物体时，一定会造成一些信息的差异。可以让低纬度也可以能正确的表示高纬度的事物，或者信息差异最小。

- 目的：特征数量达到上百，上千的时候，考虑数据的优化。使数据维度压缩，尽可能降低源数据的维度（复杂度），损失少量信息。

- 作用：可以削减回归分析或者聚类分析中特征的数量

- PCA大致原理

  ![1586764351314](asserts/1586764351314.png)

  - 红色为原始的样本特征，为二维的特征，如果降为一维，则可以将5个红色的原始特征，映射到一维的线段上就变成了4个特征。

  - PCA语法

    - from sklearn.decomposition import PCA
    - pca = PCA(n_components=None)
      - n_components可以为小数（保留特征的百分比），整数（减少到的特征数量）
    - pca.fit_transform(X)

    ```python
    from sklearn.decomposition import PCA
    data = [[0,2,4,3],[0,3,7,3],[0,9,6,3]] #4维特征，4种特征
    pca = PCA(n_components=3)
    pca.fit_transform(data)
    ---------------
    array([[-2.88362421e+00, -1.25443227e+00,  2.03388303e-16],
           [-1.45140588e+00,  1.56492061e+00,  2.03388303e-16],
           [ 4.33503009e+00, -3.10488337e-01,  2.03388303e-16]])
    ```

    