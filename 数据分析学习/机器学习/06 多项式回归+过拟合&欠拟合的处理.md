## 案例分析

+ 房地产估价数据集数据集（house.xlsx）

  - 数据集信息

    > 房地产估值的市场历史数据集来自台湾新北市新店区。“房地产估价” 是一个回归问题。

  - 属性信息

    - 输入如下

      - X1 =交易日期（例如，2013.250 = 2013年3月，2013.500 = 2013年6月，等等）
      - X2 =房屋年龄（单位：年）
      - X3 =到最近的捷运站的距离（单位：米） ）
      - X4 =步行生活圈中的便利店的数量（整数）
      - X5 =地理坐标，纬度。（单位：度）
      - X6 =地理坐标，经度。（单位：度）

    - 输出结果如下

      > Y =单位面积的房价（10000新台币/ Ping，其中Ping是本地单位，1 Ping = 3.3米平方）

+ 使用线性回归训练模型

  + 加载数据

    ```python
    import pandas as pd
    from sklearn.metrics import r2_score,mean_squared_error as MSE
    from sklearn.linear_model import LinearRegression
    from sklearn.model_selection import train_test_split
    data = pd.read_excel('./house.xlsx').drop(labels='No',axis=1)
    data.head()
    ```

    ![1587020279595](asserts/1587020279595.png)

  + 提取特征数据及目标数据并进行拆分

    ```python
    feature = data.loc[:,data.columns != 'Y house price of unit area']
    target = data['Y house price of unit area']
    #数据集的拆分
    x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.2,random_state=2020)
    ```

  + 模型训练

    ```python
    linner = LinearRegression(fit_intercept=True,normalize=True,copy_X=True)
    linner.fit(x_train,y_train)
    ```

  + 模型评估

    + 检测模型在训练集的表现

      ```python
      MSE(y_train,linner.predict(x_train)) #83.00347064630697
      y_train.max() #117.5
      y_train.min() #7.6
      r2_score(y_train,linner.predict(x_train)) #0.5750984249253515
      ```

    + 在测试集的表现结果

      ```python
      MSE(y_test,linner.predict(x_test)) #55.33406050090861
      y_test.max() #63.2
      y_test.min() #13.8
      r2_score(y_test,linner.predict(x_test)) #0.6108181277767407
      ```

+ 结论：上述的样本训练好的模型在测试集和训练集表现的都很糟糕！怎么办呢？

  - 更换模型
  - 坚持使用线性模型（多项式模型）

+ 问题：训练好的模型在训练集上表现的预测效果很好，但是在测试集上却有很大的问题和误差，why？

  - 案例1
    - 现在有一组天鹅的特征数据然后对模型进行训练，然后模型学习到的内容是有翅膀，嘴巴长的就是天鹅。然后使用模型进行预测，该模型可能会将所有符合这两个特征的动物都预测为天鹅，则肯定会有误差的，因为鹦鹉，秃鹫都符合有翅膀和嘴巴长的特征。
    - 原因：模型学习到的天鹅的特征太少了，导致区分标准太粗糙，不能准确的识别出天鹅。
  - 案例2
    - 更新了样本的特征数据了，增加了一些特征，然后训练模型。模型这次学习到的内容是，有翅膀、嘴巴长、白色、体型像2、脖子长且有弯度的就是天鹅。然后开始使用模型进行预测，现在一组测试数据为鹦鹉，因为鹦鹉的体型小，脖子短不符合天鹅的特征，则预测结果为不是天鹅。然后又有一组特征为黑天鹅，结果因为颜色不是白色，预测成了黑天鹅。
    - 原因：现在模型学习到的特征已经基本可以区分天鹅和其他动物了。但是学习到的特征中有一项是羽毛是白色，那么就会导致将黑天鹅无法识别出来。也就是机器学习到的特征太依赖或者太符合训练数据了。

+ 欠拟合&&过拟合

  ![1587021121516](asserts/1587021121516.png)

  + 欠拟合：案例1中的场景就可以表示欠拟合

    > 一个假设在训练数据上不能获得很好的拟合，但是在训练数据以外的数据集上也不能很好的拟合数据，此时认为这个假设出现了欠拟合的现象。（模型过于简单）

  + 过拟合：案例2中的场景就可以表示过拟合

    > 一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据以外的数据集上却不能很好的拟合数据，此时认为这个假设出现了过拟合现象。（模型过于复杂）

+ 欠拟合和过拟合的解决方案

  - 欠拟合
    - 原因：模型学习到样本的特征太少
    - 解决：增加样本的特征数量（多项式回归）
  - 过拟合：
    - 原因：原始特征过多，存在一些嘈杂特征。
    - 解决
      - 进行特征选择，消除关联性大的特征（很难做）
      - 正则化之岭回归（掌握）

## 欠拟合的处理：多项式回归

+ 模型的复杂度--》回归出直线or曲线
  - 我们的回归模型最终回归出的一定是直线吗(y=wx+b)？有没有可能是曲线（非线性）呢（y=wx**2+b）？
    - 我们都知道回归模型算法就是在寻找特征值和目标值之间存在的某种关系，那么这种关系越复杂则表示训练出的模型的复杂度越高，反之越低。
    - 模型的复杂度是由特征和目标之间的关系导致的！特征和目标之间的关系不仅仅是线性关系！
+ 为了解决欠拟合的情 经常要提高线性的次数（高次多项式）建立模型拟合曲线，次数过高会导致过拟合，次数不够会欠拟合。
  - y = w*x + b 一次多项式函数
  - y = w1*x^2 + w2*x + b 二次多项式函数
  - y = w1*x^3 + w2*x^2 + w3*x + b 三次多项式函数
  - 。。。
+ 相对于线性回归模型`y=wx+b`只能解决线性(回归出的为直线)问题，多项式回归能够解决非线性回归（回归出的为曲线）问题。
+ 拿最简单的线性模型来说，其数学表达式可以表示为：`y=wx+b`，它表示的是一条直线，而多项式回归则可以表示成：`y=w1x∧2+w2x+b`,它表示的是二次曲线，实际上，多项式回归可以看成特殊的线性模型，即把x∧2看成一个特征，把x看成另一个特征，这样就可以表示成y=w1z+w2x+b,其中z=x∧2,这样多项式回归实际上就变成线性回归了。
+ 其中的y=w1x∧2+w2x+b就是所谓的二次多项式:aX∧2+bX+c(a≠0).

- 当然还可以将y=wx+b转为更高次的多项式。是否需要转成更高次的多项式取决于我们想要拟合样本的程度了，更高次的多项式可以更好的拟合我们的样本数据，但是也不是一定的，很可能会造成过拟合。

- 高次项的次越大，则拟合出的曲线越弯曲

  - 曲线越弯曲就可以表示更好的拟合效果
  - 在样本中添加的高此项的特征的次越高，则模型会有更好的拟合。
  - 注意：如果次无限增大，则一定会出现模型的过拟合。

- 示例---模拟根据蛋糕的直径大小预测蛋糕价格

  - 一次线性回归预测

    ```python
    from sklearn.linear_model import LinearRegression
    import numpy as np
    import matplotlib.pyplot as plt
    # 样本的训练数据，特征和目标值
    x_train = [[6], [8], [10], [14], [18]] #大小
    y_train = [[7], [9], [13], [17.5], [18]]#价格
    #一次线性回归的学习与预测y=wx+b
    regressor = LinearRegression()
    regressor.fit(x_train, y_train)
    #画出一次线性回归的拟合曲线
    xx = np.linspace(0, 25, 100).reshape(-1, 1)
    yy = regressor.predict(xx)  
    plt.scatter(x_train, y_train)#原始样本数据的分布规律  
    plt.plot(xx, yy)
    ```

    ![1587022179656](asserts/1587022179656.png)

  - 建立二次多项式线性回归模型进行预测

    - 根据二次多项式公式可知，需要给原始特征添加更高次的特征数据x^2.

      - y=w1x∧2+w2x+b

    - 如何给样本添加高次的特征数据呢？

      - 使用sklearn.preprocessing.PolynomialFeatures来进行更高次特征的构造
        - 它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）
        - PolynomialFeatures有三个参数
          - degree：控制多项式的度
          - interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2^ 和b^2^。
          - include_bias：默认为True。如果为False的话，那么就不会有上面的1那一项

    - 基本使用

      ```python
      #PolynomialFeatures基本使用
      from sklearn.preprocessing import PolynomialFeatures
      c=[[5,3]]#c=[[a,b]] 初始特征
      pl=PolynomialFeatures(degree=3) #实例化工具类对象
      b=pl.fit_transform(c)
      b
      #array([[  1.,   5.,   3.,  25.,  15.,   9., 125.,  75.,  45.,  27.]])
      ```

    - 建立二次多项式线性回归模型进行预测

      - fit_transform是基于一组数据寻找规律，然后将规律作用到原始数据中
      - transform使用，上一步fit_transform已经找到的规律作用到另一组数据中

      ```python
      poly2 = PolynomialFeatures(degree=2)#2次多项式特征生成器
      x_train_poly2 = poly2.fit_transform(x_train)
      # 建立模型预测
      regressor_poly2 = LinearRegression()
      regressor_poly2.fit(x_train_poly2, y_train)
      #画二次多项式线性回归的图
      xx_poly2 = poly2.transform(xx)
      yy_poly2 = regressor_poly2.predict(xx_poly2)
      plt.scatter(x_train, y_train)
      plt.plot(xx, yy)
      plt.plot(xx, yy_poly2)
      ```

      ![1587022528849](asserts/1587022528849.png)

    - 建立3次多项式线性回归模型进行预测

      ```python
      poly3 = PolynomialFeatures(degree=3)#3次多项式特征生成器
      x_train_poly3 = poly3.fit_transform(x_train)
      # 建立模型预测
      regressor_poly3 = LinearRegression()
      regressor_poly3.fit(x_train_poly3, y_train)
      #画二次多项式线性回归的图
      xx_poly3 = poly3.transform(xx)
      yy_poly3 = regressor_poly3.predict(xx_poly3)
      plt.scatter(x_train, y_train)
      plt.plot(xx, yy)
      plt.plot(xx, yy_poly3)
      ```

      ![1587022569287](asserts/1587022569287.png)

- 基于文章开始案例使用多项式回归处理欠拟合的情况

  - 加载数据与数据拆分

    ```python
    data = pd.read_excel('./house.xlsx').drop(labels='No',axis=1)
    feature = data.loc[:,data.columns != 'Y house price of unit area']
    target = data['Y house price of unit area']
    #数据集的拆分
    x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.2,random_state=2020)
    ```

  - 给特征数据添加高次项特征数据

    ```python
    from sklearn.preprocessing import PolynomialFeatures
    pl=PolynomialFeatures(degree=2) #使用工具类给特征数据添加高次项特征数据
    ploy_x_train=pl.fit_transform(x_train) #工具类就返回了添加好的高次项的特征
    ploy_x_test=pl.transform(x_test)
    ```

  - 训练模型

    ```python
    linner = LinearRegression()
    linner.fit(ploy_x_train,y_train)
    ```

  - 模型评估

    - 检测模型在训练集的表现

      ```python
      MSE(y_train,linner.predict(ploy_x_train)) #83.0034-->60.80409445015681
      y_train.max() #117.5
      y_train.min() #7.6
      r2_score(y_train,linner.predict(ploy_x_train)) #0.575098-->0.6887388
      ```

    - 在测试集的表现结果

      ```python
      MSE(y_test,linner.predict(ploy_x_test)) #55.3340-->30.830183307250028
      y_test.max() #63.2
      y_test.min() #13.8
      r2_score(y_test,linner.predict(ploy_x_test)) #0.6108-->0.7831616123616155
      ```

  - 结论：预测效果大大增强

  - 交叉验证

    ```python
    from sklearn.model_selection import cross_val_score
    s2_dic_train = {}
    s2_dic_test = {}
    for i in range(1,20):
        pl=PolynomialFeatures(degree=i)
        ploy_x_train=pl.fit_transform(x_train)
        ploy_x_test=pl.transform(x_test)
        linner = LinearRegression()
        s2_dic_train[i] = cross_val_score(linner,ploy_x_train,y_train,cv=10,scoring='r2').mean()
        s2_dic_test[i] = cross_val_score(linner,ploy_x_test,y_test,cv=10,scoring='r2').mean()
    s1 = pd.Series(s2_dic_train)
    s2 = pd.Series(s2_dic_test)
    print(s1.argmax(),s1.max())
    print(s2.argmax(),s2.max())
    #绘制学习曲线
    import matplotlib.pyplot as plt
    plt.plot(s1)
    plt.ylim(-1,1)
    plt.xlabel('degree')
    plt.ylabel('s2_value')
    ```

    ![1587025370751](asserts/1587025370751.png)

    ```python
    plt.plot(s2)
    plt.ylim(-1,1)
    plt.xlabel('degree')
    plt.ylabel('s2_value')
    ```

    ![1587025397634](asserts/1587025397634.png)

## 过拟合处理:正则化

+ 将过拟合的曲线的凹凸幅度减少就可以将过拟合曲线趋近于拟合曲线了。那么过拟合曲线的凹凸肯定是由y=wx**2+x**3+x**4中的高次项导致的，那么正则化就可以通过不断的尝试发现高次项的特征然后这些特征的权重w调小到0，则高次项的特征没有了，那么凹凸幅度就减少了，就越趋近于拟合曲线了！

+ 可以使得高次项的w权重减小，趋近于0.

+ LinnerRegression是没有办法进行正则化的，所以该算法模型容易出现过拟合，并且无法解决。

+ L2正则化

  > 使用带有正则化算法的回归模型（Ridge岭回归）处理过拟合的问题。

### Ridge岭回归:具备L2正则化的线性回归模型

- API:from sklearn.linear_model import Ridge

- Ridge(alpha=1.0)

  - alpha:正则化的力度，力度越大，则表示高次项的权重w越接近于0，导致过拟合曲线的凹凸幅度越小。

    > 取值：0-1小数或者1-10整数

  - coef_:回归系数

- 使用示例

  - 样本数据

    ```python
    from sklearn.linear_model import LinearRegression
    # 样本的训练数据，特征和目标值
    x_train = [[6], [8], [10], [14], [18]] #大小
    y_train = [[7], [9], [13], [17.5], [18]]#价格
    ```

  - 未使用时

    ```python
    from sklearn.preprocessing import PolynomialFeatures
    poly3 = PolynomialFeatures(degree=3)#3次多项式特征生成器
    x_train_poly3 = poly3.fit_transform(x_train)
    # 建立模型预测
    regressor_poly3 = LinearRegression()
    regressor_poly3.fit(x_train_poly3, y_train)
    regressor_poly3.coef_
    #array([[ 0.        , -1.42626096,  0.31320489, -0.01103344]])
    ```

  - 使用岭回归通过控制正则化力度参数alpha降低高次项特征的权重

    ```python
    from sklearn.linear_model import Ridge
    poly3 = PolynomialFeatures(degree=3)#3次多项式特征生成器
    x_train_poly3 = poly3.fit_transform(x_train)
    # 建立模型预测
    regressor_poly3 = Ridge(alpha=0.5)
    regressor_poly3.fit(x_train_poly3, y_train)
    regressor_poly3.coef_
    #array([[ 0.        , -0.14579637,  0.19991159, -0.00792083]])
    ```

- 岭回归的优点

  - 获取的回归系数更符合实际更可靠
  - 在病态数据（异常值多的数据）偏多的研究中有更大的存在意义

## 模型的保存和加载

- from sklearn.externals import joblib
  - joblib.dump(model,'xxx.m'):保存
  - joblib.load('xxx.m'):加载